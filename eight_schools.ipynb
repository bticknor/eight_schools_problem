{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dda51a",
   "metadata": {},
   "source": [
    "# The Eight Schools Problem: A Hierarchical Bayesian Analysis\n",
    "\n",
    "The \"eight schools problem\" is a very famous example problem that is often used to illustrate hierarchical models and the Bayesian inference process.  This problem was originally described in a 1981 paper by TODO ref, and it has prominently been features as an example in the third edition of Bayesian Data Analysis (TODO reference), to which I will refer below as \"BDA3.\"  The \"full Bayesian\" model that is commonly used to analyze this problem is simple but very generalizable, and a good understanding of how this model is constructed and analyzed provides a lot of insight into Bayesian data analysis.  Unfortunately, while there are several STAN implementations of this problem available on the internet, none of them provide a lot of context about the problem and simply refer the reader to BDA3.  The **purpose of this document** is to give a reasonably in-depth description of the eight schools problem and the hierarchical model that is commonly used to analyze it.  The document includes:\n",
    "\n",
    "- An introduction of the eight schools problem and associated data\n",
    "\n",
    "\n",
    "- A defintion of the hierarchical model used to do analyses\n",
    "\n",
    "\n",
    "- A partial analytical derivation of the full model posterior\n",
    "\n",
    "\n",
    "- A PySTAN implementation of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7239e",
   "metadata": {},
   "source": [
    "## The Effects of SAT-V Coaching in Eight Schools \n",
    "\n",
    "Eight schools hosted special coaching programs for the verbal section of the Scholastic Aptitude Test (SAT-V), and a study was done to investigate the effects of these programs.  For each school, a linear regression was performed where student SAT-V scores were regressed on a binary variable indicating treatment group, with student scores on multiple PSAT sections included as additional controls.  The effects measured by these regresions, along with their sample variances - which are assumed known - are summarized in this table:\n",
    "\n",
    "\n",
    "| School | Estimated Treatment Effect | Effect Standard Error |\n",
    "| --- | --- | --- |\n",
    "| 1 | 28 | 15 |\n",
    "| 2 | 8 | 10 |\n",
    "| 3 | -3 | 16 |\n",
    "| 4 | 7 | 11 |\n",
    "| 5 | -1 | 9 |\n",
    "| 6 | 1 | 11 |\n",
    "| 7 | 18 | 10 |\n",
    "| 8 | 12 | 18 |\n",
    "\n",
    "\n",
    "The problem we now face is how to interpret these results and use them to do inference.  One thing we could do is to assume that there is no difference in the effect between schools, and therefore that each school's measured effect is an independent estimate of the one \"common\" effect $\\theta$.  Suppose that we do this and perform a simple Bayesian analysis where our estimates across schools are assumed to be normally distributed and a noninformative (uniform) prior distribution is given to $\\theta$.  We will get that our posterior mean for $\\theta$ is ~7.7.  Consider a second simple strategy where we assume that each school's effect is different, and we only use data from a given school to estimate the effect at that school.  A simple Bayesian analysis for each school yields school level posterior means that are close to the observed effect sizes, but which are essentially indistinguishable based on the widths of the posterior credible intervals.\n",
    "\n",
    "These two strategies are nice because of their simplicity, but they are each rather blunt.  The first uses a complete pooling of information across schools, and the second does not allow for any information sharing across schools.  These two strategies also give very different estimates for some of the school level effects.  For example, the first simple strategy estimates the posterior mean of the effect at school 1 as $\\approx 7.7$ (the pooled estimate given to all schools), while the second simple strategy estimates it as $\\approx 28$.  This result motivates the use of a slightly more complicated model that allows for the \"partial\" sharing of information across schools...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11ea74",
   "metadata": {},
   "source": [
    "## Hierarchical Generative Model\n",
    "\n",
    "Let $\\bar{y}_{.j}$ denote the estimated treatment effect for school $j$, where $j=1, ..., 8$.  Let $\\sigma_j^2$ denote the sampling variance associated with the estimate at school $j$.  Note that this notation is used to maintain consistency with the presentation of this problem in BDA3, and the \"dot\" notation used to denote the within group sample mean is classically used in analysis of variance.  We now assume the following hierarchical model describing the generative process that creates these $\\theta_j$ values:\n",
    "\n",
    "$$\n",
    "\\overline{y_{.j}} \\sim N(\\theta_j, \\sigma_j^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\overline{y_{.j}} = \\frac{1}{n_j} \\sum_{i=1}^{n_j} y_{ij}\n",
    "$$\n",
    "$$\n",
    "\\sigma_j^2 = \\frac{\\sigma^2}{n_j}\n",
    "$$\n",
    "\n",
    "Here comes the hierarchical part - we assume that the $\\theta_j$ values are themselves sampled independently from a Normal distribution with shared hyperparameters $\\mu$ and $\\tau$:\n",
    "\n",
    "$$\n",
    "\\theta_j \\sim_{iid} N(\\mu, \\tau^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327329e",
   "metadata": {},
   "source": [
    "## Posterior Distribution over Model Parameters\n",
    "\n",
    "Using a Bayesian approach with the model specified above, the full posterior distribution over all parameters can be factorized using the probability chain rule:\n",
    "\n",
    "$p(\\theta, \\mu, \\tau | y) = p(\\theta|\\mu, \\tau, y)p(\\mu|\\tau, y)p(\\tau|y)$\n",
    "\n",
    "\n",
    "Specifying $p(\\mu, \\tau) = p(\\mu | \\tau)p(\\tau)$ - the prior distribution over our hyperparameters - will give us enough information to analytically solve for the posterior.  We will do this below as we derive the second and third terms of the factorization.  We don't need any information from the prior in order to derive the first term $p(\\theta|\\mu, \\tau, y)$, since we are already conditioning on both $\\mu$ and $\\tau$.  Our model assumptions give us a prior for each $\\theta_j$, and a sampling distribution of $y_{ij}$ given $\\theta_j$ (a likelihood).  Using these to solve for the posterior on $\\theta$ yields:\n",
    "\n",
    "$$\n",
    "\\theta|\\mu, \\tau, y \\sim N(\\hat{\\theta}_j, V_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_j = \\frac{\\frac{1}{\\sigma_j^2} \\bar{y}_{.j} + \\frac{1}{\\tau^2} \\mu  }{\\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_j = \\frac{1}{ \\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}  }\n",
    "$$\n",
    "\n",
    "Now, we **assume a uniform conditional prior** on $\\mu|\\tau$, or in other words assume that $p(\\mu | \\tau) = c$.  This allows us to factor the joint prior over these hyperparameters as $p(\\mu, \\tau) = p(\\mu|\\tau)p(\\tau) \\propto p(\\tau)$.  The assignment of a \"noninformative\" prior over $\\mu|\\tau$ is a **modeling decision** that is motivated by the fact that the data we observe will provide us with a lot of information about $\\mu$ (pg. 115).  This decision implies the following conditional distribution for $\\mu|\\tau, y$, the second term in the factorization of the posterior:\n",
    "\n",
    "$$\n",
    "\\mu|\\tau, y \\sim N(\\hat{\\mu}, V_\\mu)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{  \\sum_{j=1}^J \\frac{1}{\\sigma^2_j + \\tau^2} \\bar{y}_{.j}    }{  \\sum_{j=1}^J \\frac{1}{\\sigma^2_j + \\tau^2}  }\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_\\mu^{-1} = \\sum_{j=1}^J \\frac{1}{\\sigma_j^2 + \\tau^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, we deterine the third term of the factorization, the conditional posterior of $\\tau|y$.  We can express this as:\n",
    "\n",
    "$$\n",
    "p(\\tau|y) = \\frac{p(\\mu, \\tau|y)}{p(\\mu|\\tau, y)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\propto \\frac{p(\\tau) \\prod_{j=1}^J N(\\bar{y}_{.j}|\\hat{\\mu}, \\sigma_j^2 + \\tau^2) }{N(\\hat{\\mu}|\\hat{\\mu}, V_u)}\n",
    "$$\n",
    "$$\n",
    "\\propto p(\\tau) V_u^{1/2} \\prod_{j=1}^J (\\sigma_j^2 + \\tau^2)^{-1/2} exp\\left(  - \\frac{(\\bar{y}_{.j} - \\hat{\\mu})^2} {2(\\sigma_j^2 + \\tau^2)} \\right)\n",
    "$$\n",
    "\n",
    "We **assume a uniform prior** on $\\tau$: $p(\\tau) \\propto c$ for $\\tau > 0$.  This implies:\n",
    "\n",
    "$$\n",
    "p(\\tau|y) \\propto V_u^{1/2} \\prod_{j=1}^J (\\sigma_j^2 + \\tau^2)^{-1/2} exp\\left(  - \\frac{(\\bar{y}_{.j} - \\hat{\\mu})^2} {2(\\sigma_j^2 + \\tau^2)} \\right)\n",
    "$$\n",
    "\n",
    "We now have an analytical expression (up to constant scaling) of our posterior $p(\\theta, \\mu, \\tau | y)$.  We see that the posterior is quite a complicated function, and instead of attempting to integrate it (e.g. to normalize it) we will prefer to analyze it numerically.  This can be done in a straightforward way - we first sample $\\tau$, which can be done by computing $p(\\tau|y)$ for a uniformly spaced grid of $\\tau$ values and using the inverse CDF method (see [here](https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html)) on this discretized distribution of $\\tau$.  We then sample $\\mu$ and then $\\theta$ from their associated Normal distributions.\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "To summarize, we:\n",
    "\n",
    "- expressed the posterior distribution for the parameters of our hierarchical model as a factorization of three conditional posteriors\n",
    "\n",
    "- Assumed noninformative uniform priors for $p(\\mu|\\tau)$ and $p(\\tau)$\n",
    "\n",
    "- Used the the assumed priors to analytically express the posterior factors\n",
    "\n",
    "\n",
    "The full details of the posterior derivation are omitted here for the sake of brevity, but are included in BDA3. (TODO REF) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29013fb4",
   "metadata": {},
   "source": [
    "## Tau and Pooled vs. Separate School Estimates  \n",
    "\n",
    "The procedure described above allows us to do inference on the $\\tau$ parameter of the model after specifying a prior distribution over it.  However, if we were to set $\\tau = 0$ we would find that the distributions for the $\\theta_j$ values given by our Bayesian procedure would all be centered around the pooled estimate of $\\theta_j = 7.7$ that is given by the the first \"simple strategy\" described above.  On the other hand, if we take $\\tau \\rightarrow \\infty$, we find that the distribution estimates over $\\theta_j$ parameters are centered about the separate estimates given by the second \"simple strategy.\"  We can thus think of $\\tau$ as a sort of knob that we could turn to give us a different interpolation between the fully pooled and fully separated strategies for estimating the $\\theta_j$ values.  This result makes sense given the interpretation of $\\tau$ in the model - small values of $\\tau$ imply that the different school effects are (in all probability) very close to each other.\n",
    "\n",
    "\n",
    "This is mentioned only as a curiosity, but it illustrates the flexibility and power of the hierarchical model we've constructed to describe this scenario.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41adfef1",
   "metadata": {},
   "source": [
    "## Automatically Sampling from the Posterior with STAN\n",
    "\n",
    "In BDA3, Gelman et. all mention the sampling method described above as a way to numerically analyze the posterior distribution of the model (pg. 118).  However, in general we can avoid the posterior derivations necessary for this by using a dedicated Bayesian software package like STAN.  STAN allows us to build a model by declaring a likelihood and prior, and then automatically do inference on the posterior after supplying data to the model.  STAN is very powerful in that it allows us to easily experiment with different model specifications without having to spend time coming up with a good way to sample from the posterior - it all happens \"automagically.\"  The magic is supplied by way of a powerful Markov Chain Monte Carlo algorithm known as Hamiltonian Monte Carlo, which is how STAN draws samples.  STAN has APIs for R and Python (among other languages), and below is included an example of using stan through the PyStan Python package.  Here are some helpful resources related to STAN and PyStan:\n",
    "\n",
    "\n",
    "- The Stan User Manual is [here](https://mc-stan.org/docs/2_29/stan-users-guide-2_29.pdf).\n",
    "\n",
    "\n",
    "- A wonderful paper that gives some intuition for Hamiltonian Monte Carlo sampling is [here](https://arxiv.org/pdf/1701.02434.pdf?ref=https://githubhelp.com)\n",
    "\n",
    "\n",
    "- The PyStan GitHub repository (which includes some documentation) can be [here](https://github.com/stan-dev/pystan).\n",
    "\n",
    "\n",
    "\n",
    "Below is a PyStan implementation of the eight schools problem described in detail above.  TODO finish blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85a1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> J;\n",
    "  real y[J];\n",
    "  real<lower=0> sigma[J];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real mu;\n",
    "  real<lower=0> tau;\n",
    "  real theta[J];\n",
    "}\n",
    "\n",
    "model {\n",
    "  mu ~ uniform(-20, 20);\n",
    "  tau ~ uniform(0, 50);\n",
    "  theta ~ normal(mu, tau);\n",
    "  y ~ normal(theta, sigma);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "model_data = dict(J = 8, y = [28,  8, -3,  7, -1,  1, 18, 12], \n",
    "sigma = [15, 10, 16, 11,  9, 11, 10, 18])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3951328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> J;\n",
    "  real y[J];\n",
    "  real<lower=0> sigma[J];\n",
    "  real<lower=0> tau;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real mu;\n",
    "  real theta[J];\n",
    "}\n",
    "\n",
    "model {\n",
    "  theta ~ normal(mu, tau);\n",
    "  y ~ normal(theta, sigma);\n",
    "  mu ~ uniform(-20, 20);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "model_data = dict(J = 8, y = [28,  8, -3,  7, -1,  1, 18, 12], \n",
    "sigma = [15, 10, 16, 11,  9, 11, 10, 18])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02e6846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'J': 8,\n",
       " 'y': [28, 8, -3, 7, -1, 1, 18, 12],\n",
       " 'sigma': [15, 10, 16, 11, 9, 11, 10, 18],\n",
       " 'tau': 8}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data[\"tau\"] = 8\n",
    "\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb1481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_04b6ca40ee67b46c0e85084ce20ced4f NOW.\n",
      "WARNING:pystan:103 of 2000 iterations ended with a divergence (5.15 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    }
   ],
   "source": [
    "# fit = pystan.stan(\n",
    "#     model_code=model_code, data=model_data, iter=1000, chains=1\n",
    "# )\n",
    "\n",
    "# compile model\n",
    "sm = pystan.StanModel(model_code=model_code)\n",
    "\n",
    "# Train the model and generate samples\n",
    "fit = sm.sampling(\n",
    "    data=model_data,\n",
    "    iter=1000,\n",
    "    chains=4,\n",
    "    warmup=500,\n",
    "    thin=1,\n",
    "    control=dict(adapt_delta=0.8),\n",
    "    seed=101\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb66825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>8.031578</td>\n",
       "      <td>0.333714</td>\n",
       "      <td>4.061936</td>\n",
       "      <td>-0.703092</td>\n",
       "      <td>5.589061</td>\n",
       "      <td>8.048183</td>\n",
       "      <td>10.684351</td>\n",
       "      <td>15.842837</td>\n",
       "      <td>148.154933</td>\n",
       "      <td>1.036538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[1]</th>\n",
       "      <td>8.138984</td>\n",
       "      <td>0.335126</td>\n",
       "      <td>4.160210</td>\n",
       "      <td>-0.834255</td>\n",
       "      <td>5.490742</td>\n",
       "      <td>8.147732</td>\n",
       "      <td>10.975592</td>\n",
       "      <td>16.493141</td>\n",
       "      <td>154.104432</td>\n",
       "      <td>1.033676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[2]</th>\n",
       "      <td>8.037414</td>\n",
       "      <td>0.338071</td>\n",
       "      <td>4.149238</td>\n",
       "      <td>-0.756605</td>\n",
       "      <td>5.413917</td>\n",
       "      <td>8.015429</td>\n",
       "      <td>10.695799</td>\n",
       "      <td>16.216609</td>\n",
       "      <td>150.633343</td>\n",
       "      <td>1.035873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[3]</th>\n",
       "      <td>7.968147</td>\n",
       "      <td>0.329338</td>\n",
       "      <td>4.156863</td>\n",
       "      <td>-0.803777</td>\n",
       "      <td>5.364213</td>\n",
       "      <td>7.969778</td>\n",
       "      <td>10.762491</td>\n",
       "      <td>15.867992</td>\n",
       "      <td>159.311289</td>\n",
       "      <td>1.033385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[4]</th>\n",
       "      <td>7.990305</td>\n",
       "      <td>0.337747</td>\n",
       "      <td>4.150151</td>\n",
       "      <td>-0.546746</td>\n",
       "      <td>5.401063</td>\n",
       "      <td>8.060295</td>\n",
       "      <td>10.682770</td>\n",
       "      <td>15.966547</td>\n",
       "      <td>150.989121</td>\n",
       "      <td>1.033497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[5]</th>\n",
       "      <td>7.905201</td>\n",
       "      <td>0.335611</td>\n",
       "      <td>4.137315</td>\n",
       "      <td>-1.045362</td>\n",
       "      <td>5.277481</td>\n",
       "      <td>7.941587</td>\n",
       "      <td>10.721696</td>\n",
       "      <td>15.857161</td>\n",
       "      <td>151.972376</td>\n",
       "      <td>1.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[6]</th>\n",
       "      <td>7.957442</td>\n",
       "      <td>0.330549</td>\n",
       "      <td>4.161165</td>\n",
       "      <td>-0.896794</td>\n",
       "      <td>5.381397</td>\n",
       "      <td>7.960650</td>\n",
       "      <td>10.699240</td>\n",
       "      <td>16.176975</td>\n",
       "      <td>158.473788</td>\n",
       "      <td>1.033722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[7]</th>\n",
       "      <td>8.112797</td>\n",
       "      <td>0.330533</td>\n",
       "      <td>4.132082</td>\n",
       "      <td>-0.598849</td>\n",
       "      <td>5.562385</td>\n",
       "      <td>8.042120</td>\n",
       "      <td>10.790248</td>\n",
       "      <td>15.955197</td>\n",
       "      <td>156.281543</td>\n",
       "      <td>1.032959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[8]</th>\n",
       "      <td>8.049164</td>\n",
       "      <td>0.341033</td>\n",
       "      <td>4.167562</td>\n",
       "      <td>-0.471013</td>\n",
       "      <td>5.497870</td>\n",
       "      <td>8.099621</td>\n",
       "      <td>10.730022</td>\n",
       "      <td>15.954249</td>\n",
       "      <td>149.338758</td>\n",
       "      <td>1.035368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-6.711166</td>\n",
       "      <td>0.091008</td>\n",
       "      <td>2.068612</td>\n",
       "      <td>-11.632739</td>\n",
       "      <td>-7.883887</td>\n",
       "      <td>-6.422469</td>\n",
       "      <td>-5.168679</td>\n",
       "      <td>-3.593750</td>\n",
       "      <td>516.651844</td>\n",
       "      <td>1.008145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean   se_mean        sd       2.5%       25%       50%  \\\n",
       "mu        8.031578  0.333714  4.061936  -0.703092  5.589061  8.048183   \n",
       "theta[1]  8.138984  0.335126  4.160210  -0.834255  5.490742  8.147732   \n",
       "theta[2]  8.037414  0.338071  4.149238  -0.756605  5.413917  8.015429   \n",
       "theta[3]  7.968147  0.329338  4.156863  -0.803777  5.364213  7.969778   \n",
       "theta[4]  7.990305  0.337747  4.150151  -0.546746  5.401063  8.060295   \n",
       "theta[5]  7.905201  0.335611  4.137315  -1.045362  5.277481  7.941587   \n",
       "theta[6]  7.957442  0.330549  4.161165  -0.896794  5.381397  7.960650   \n",
       "theta[7]  8.112797  0.330533  4.132082  -0.598849  5.562385  8.042120   \n",
       "theta[8]  8.049164  0.341033  4.167562  -0.471013  5.497870  8.099621   \n",
       "lp__     -6.711166  0.091008  2.068612 -11.632739 -7.883887 -6.422469   \n",
       "\n",
       "                75%      97.5%       n_eff      Rhat  \n",
       "mu        10.684351  15.842837  148.154933  1.036538  \n",
       "theta[1]  10.975592  16.493141  154.104432  1.033676  \n",
       "theta[2]  10.695799  16.216609  150.633343  1.035873  \n",
       "theta[3]  10.762491  15.867992  159.311289  1.033385  \n",
       "theta[4]  10.682770  15.966547  150.989121  1.033497  \n",
       "theta[5]  10.721696  15.857161  151.972376  1.035300  \n",
       "theta[6]  10.699240  16.176975  158.473788  1.033722  \n",
       "theta[7]  10.790248  15.955197  156.281543  1.032959  \n",
       "theta[8]  10.730022  15.954249  149.338758  1.035368  \n",
       "lp__      -5.168679  -3.593750  516.651844  1.008145  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_dict = fit.summary()\n",
    "df = pd.DataFrame(summary_dict['summary'], \n",
    "                  columns=summary_dict['summary_colnames'], \n",
    "                  index=summary_dict['summary_rownames'])\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef031aa1",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "TODO!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
