{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dda51a",
   "metadata": {},
   "source": [
    "# The Eight Schools Problem: An Intro to Hierarchical Bayesian Analysis\n",
    "\n",
    "The \"eight schools problem\" is a very famous example problem that is often used to illustrate hierarchical models and the Bayesian inference process.  This problem was originally described in a 1981 paper by Donald Rubin [2], and it has prominently been features as an example in the third edition of Bayesian Data Analysis [1], to which I will refer below as \"BDA3.\"  The \"full Bayesian\" model that is commonly used to analyze this problem is simple but very generalizable, and a good understanding of how this model is constructed and analyzed provides a lot of insight into Bayesian data analysis.  Unfortunately, while there are several STAN implementations of this problem available on the internet, none of them provide a lot of context about the problem and simply refer the reader to BDA3.  The **purpose of this document** is to give a reasonably in-depth description of the eight schools problem and the hierarchical model that is commonly used to analyze it.  The document includes:\n",
    "\n",
    "- An introduction of the eight schools problem and associated data\n",
    "\n",
    "\n",
    "- A definition of the hierarchical model used to do analysis\n",
    "\n",
    "\n",
    "- An incomplete analytical derivation of the full model posterior distribution\n",
    "\n",
    "\n",
    "- A PySTAN implementation of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7239e",
   "metadata": {},
   "source": [
    "## The Effects of SAT-V Coaching in Eight Schools \n",
    "\n",
    "Eight schools hosted special coaching programs for the verbal section of the Scholastic Aptitude Test (SAT-V), and a study was done to investigate the effects of these programs.  For each school, a linear regression was performed where student SAT-V scores were regressed on a binary variable indicating treatment group, with student scores on multiple PSAT sections included as additional controls.  The effects measured by these regresions, along with their sample variances - which are assumed known - are summarized in this table:\n",
    "\n",
    "\n",
    "| School | Estimated Treatment Effect | Effect Standard Error |\n",
    "| --- | --- | --- |\n",
    "| 1 | 28 | 15 |\n",
    "| 2 | 8 | 10 |\n",
    "| 3 | -3 | 16 |\n",
    "| 4 | 7 | 11 |\n",
    "| 5 | -1 | 9 |\n",
    "| 6 | 1 | 11 |\n",
    "| 7 | 18 | 10 |\n",
    "| 8 | 12 | 18 |\n",
    "\n",
    "\n",
    "The problem we now face is how to interpret these results and use them to do inference.  One thing we could do is to assume that there is no difference in the effect between schools, and therefore that each school's measured effect is an independent estimate of the one \"common\" effect $\\theta$.  Suppose that we do this and perform a simple Bayesian analysis where our estimates across schools are assumed to be normally distributed and a noninformative (uniform) prior distribution is given to $\\theta$.  We will get that our posterior mean for $\\theta$ is ~7.7.  Consider a second simple strategy where we assume that each school's effect is different, and we only use data from a given school to estimate the effect at that school.  A simple Bayesian analysis for each school yields school level posterior means that are close to the observed effect sizes, but which are essentially indistinguishable based on the widths of the posterior credible intervals.\n",
    "\n",
    "These two strategies are nice because of their simplicity, but they are each rather blunt.  The first uses a complete pooling of information across schools, and the second does not allow for any information sharing across schools.  These two strategies also give very different estimates for some of the school level effects.  For example, the first simple strategy estimates the posterior mean of the effect at school 1 as $\\approx 7.7$ (the pooled estimate given to all schools), while the second simple strategy estimates it as $\\approx 28$.  This result motivates the use of a slightly more complicated model that allows for the \"partial\" sharing of information across schools...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad11ea74",
   "metadata": {},
   "source": [
    "## Hierarchical Generative Model\n",
    "\n",
    "Let $\\bar{y}_{.j}$ denote the estimated treatment effect for school $j$, where $j=1, ..., 8$.  Let $\\sigma_j^2$ denote the sampling variance associated with the estimate at school $j$.  Note that this notation is used to maintain consistency with the presentation of this model in BDA3, and the \"dot\" notation used to denote the within group sample mean is classically used in analysis of variance.  We now assume the following hierarchical model describing the generative process that creates these $\\theta_j$ values:\n",
    "\n",
    "$$\n",
    "\\overline{y_{.j}} \\sim N(\\theta_j, \\sigma_j^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\overline{y_{.j}} = \\frac{1}{n_j} \\sum_{i=1}^{n_j} y_{ij}\n",
    "$$\n",
    "$$\n",
    "\\sigma_j^2 = \\frac{\\sigma^2}{n_j}\n",
    "$$\n",
    "\n",
    "Here comes the hierarchical part - we assume that the $\\theta_j$ values are themselves sampled independently from a Normal distribution with shared hyperparameters $\\mu$ and $\\tau$:\n",
    "\n",
    "$$\n",
    "\\theta_j \\sim_{iid} N(\\mu, \\tau^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327329e",
   "metadata": {},
   "source": [
    "## Posterior Distribution over Model Parameters\n",
    "\n",
    "Using a Bayesian approach with the model specified above, the full posterior distribution over all parameters can be factorized using the probability chain rule:\n",
    "\n",
    "$p(\\theta, \\mu, \\tau | y) = p(\\theta|\\mu, \\tau, y)p(\\mu|\\tau, y)p(\\tau|y)$\n",
    "\n",
    "\n",
    "Specifying $p(\\mu, \\tau) = p(\\mu | \\tau)p(\\tau)$ - the prior distribution over our hyperparameters - will give us enough information to analytically solve for the posterior.  We will do this below as we derive the second and third terms of the factorization.  We don't need any information from the prior in order to derive the first term $p(\\theta|\\mu, \\tau, y)$, since we are already conditioning on both $\\mu$ and $\\tau$.  Our model assumptions give us a prior for each $\\theta_j$, and a sampling distribution of $y_{ij}$ given $\\theta_j$ (a likelihood).  Using these to solve for the posterior on $\\theta$ yields:\n",
    "\n",
    "$$\n",
    "\\theta|\\mu, \\tau, y \\sim N(\\hat{\\theta}_j, V_j)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_j = \\frac{\\frac{1}{\\sigma_j^2} \\bar{y}_{.j} + \\frac{1}{\\tau^2} \\mu  }{\\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_j = \\frac{1}{ \\frac{1}{\\sigma_j^2} + \\frac{1}{\\tau^2}  }\n",
    "$$\n",
    "\n",
    "Now, we **assume a uniform conditional prior** on $\\mu|\\tau$, or in other words assume that $p(\\mu | \\tau) = c$.  This allows us to factor the joint prior over these hyperparameters as $p(\\mu, \\tau) = p(\\mu|\\tau)p(\\tau) \\propto p(\\tau)$.  The assignment of a \"noninformative\" prior over $\\mu|\\tau$ is a **modeling decision** that is motivated by the fact that the data we observe will provide us with a lot of information about $\\mu$ (pg. 115).  This decision implies the following conditional distribution for $\\mu|\\tau, y$, the second term in the factorization of the posterior:\n",
    "\n",
    "$$\n",
    "\\mu|\\tau, y \\sim N(\\hat{\\mu}, V_\\mu)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{  \\sum_{j=1}^J \\frac{1}{\\sigma^2_j + \\tau^2} \\bar{y}_{.j}    }{  \\sum_{j=1}^J \\frac{1}{\\sigma^2_j + \\tau^2}  }\n",
    "$$\n",
    "\n",
    "$$\n",
    "V_\\mu^{-1} = \\sum_{j=1}^J \\frac{1}{\\sigma_j^2 + \\tau^2}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, we deterine the third term of the factorization, the conditional posterior of $\\tau|y$.  We can express this as:\n",
    "\n",
    "$$\n",
    "p(\\tau|y) = \\frac{p(\\mu, \\tau|y)}{p(\\mu|\\tau, y)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\propto \\frac{p(\\tau) \\prod_{j=1}^J N(\\bar{y}_{.j}|\\hat{\\mu}, \\sigma_j^2 + \\tau^2) }{N(\\hat{\\mu}|\\hat{\\mu}, V_u)}\n",
    "$$\n",
    "$$\n",
    "\\propto p(\\tau) V_u^{1/2} \\prod_{j=1}^J (\\sigma_j^2 + \\tau^2)^{-1/2} exp\\left(  - \\frac{(\\bar{y}_{.j} - \\hat{\\mu})^2} {2(\\sigma_j^2 + \\tau^2)} \\right)\n",
    "$$\n",
    "\n",
    "We **assume a uniform prior** on $\\tau$: $p(\\tau) \\propto c$ for $\\tau > 0$.  This implies:\n",
    "\n",
    "$$\n",
    "p(\\tau|y) \\propto V_u^{1/2} \\prod_{j=1}^J (\\sigma_j^2 + \\tau^2)^{-1/2} exp\\left(  - \\frac{(\\bar{y}_{.j} - \\hat{\\mu})^2} {2(\\sigma_j^2 + \\tau^2)} \\right)\n",
    "$$\n",
    "\n",
    "We now have an analytical expression (up to constant scaling) of our posterior $p(\\theta, \\mu, \\tau | y)$.  We see that the posterior is quite a complicated function, and instead of attempting to integrate it (e.g. to normalize it) we will prefer to analyze it numerically.  This can be done in a straightforward way - we first sample $\\tau$, which can be done by computing $p(\\tau|y)$ for a uniformly spaced grid of $\\tau$ values and using the inverse CDF method (see [here](https://stephens999.github.io/fiveMinuteStats/inverse_transform_sampling.html)) on this discretized distribution of $\\tau$.  We then sample $\\mu$ and then $\\theta$ from their associated Normal distributions.\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "To summarize, we:\n",
    "\n",
    "- expressed the posterior distribution for the parameters of our hierarchical model as a factorization of three conditional posteriors\n",
    "\n",
    "- Assumed noninformative uniform priors for $p(\\mu|\\tau)$ and $p(\\tau)$\n",
    "\n",
    "- Used the the assumed priors to analytically express the posterior factors\n",
    "\n",
    "\n",
    "The full details of the posterior derivation are omitted here for the sake of brevity, but are included in chapter 5 ofi BDA3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29013fb4",
   "metadata": {},
   "source": [
    "## Tau and Pooled vs. Separate School Estimates  \n",
    "\n",
    "The procedure described above allows us to do inference on the $\\tau$ parameter of the model after specifying a prior distribution over it.  However, if we were to set $\\tau = 0$ we would find that the distributions for the $\\theta_j$ values given by our Bayesian procedure would all be centered around the pooled estimate of $\\theta_j = 7.7$ that is given by the the first \"simple strategy\" described above.  On the other hand, if we take $\\tau \\rightarrow \\infty$, we find that the distribution estimates over $\\theta_j$ parameters are centered about the separate estimates given by the second \"simple strategy.\"  We can thus think of $\\tau$ as a sort of knob that we could turn to give us a different interpolation between the fully pooled and fully separated strategies for estimating the $\\theta_j$ values.  This result makes sense given the interpretation of $\\tau$ in the model - small values of $\\tau$ imply that the different school effects are (in all probability) very close to each other.\n",
    "\n",
    "\n",
    "This is mentioned only as a curiosity, but it illustrates the flexibility and power of the hierarchical model we've constructed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41adfef1",
   "metadata": {},
   "source": [
    "## Automatically Sampling from the Model Posterior with STAN\n",
    "\n",
    "In BDA3, Gelman et. all mention the sampling method described above as a way to numerically analyze the posterior distribution of the model ([1], pg. 118).  However, in general we can avoid the posterior derivations necessary for this by using a dedicated Bayesian software package like STAN.  STAN allows us to build a model by declaring a likelihood and prior, and then automatically sample from the posterior after supplying data to the model.  STAN is very powerful in that it allows us to easily experiment with different model specifications without having to spend time coming up with a good way to sample from the posterior - it all happens \"automagically.\"  The magic is supplied by way of a powerful Markov Chain Monte Carlo algorithm known as Hamiltonian Monte Carlo, which is how STAN draws samples.  STAN has APIs for R and Python (among other languages), and PyStan is used below to analyze the hierarchical model described above.\n",
    "\n",
    "In the code below, the priors on $\\mu$ and $\\tau$ are not uniform, as they are assumed in the general model presented in BDA3.  Rather, these are the priors suggested in the original Rubin paper [2]. Unfortunately, these priors give the posterior a geometry that is problematic for the STAN sampler, which is evidenced by the divergence warnings we get when we sample.  This illustrates the difficulty of modeling with a program as flexible as STAN - we can end up building models that yield posteriors that are very difficult to sample from.  [Here](https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html) is a phenomenal blog post that goes into depth about how to diagnose and address the issues that arise when using STAN to do inference.\n",
    "\n",
    "\n",
    "\n",
    "Here are some additional helpful resources related to STAN and PyStan:\n",
    "\n",
    "\n",
    "- The Stan User Manual is [here](https://mc-stan.org/docs/2_29/stan-users-guide-2_29.pdf).\n",
    "\n",
    "\n",
    "- A wonderful paper that gives some intuition for Hamiltonian Monte Carlo sampling is [here](https://arxiv.org/pdf/1701.02434.pdf?ref=https://githubhelp.com)\n",
    "\n",
    "\n",
    "- The PyStan GitHub repository (which includes some documentation) can be [here](https://github.com/stan-dev/pystan).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a85a1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_3955c9c7b33c6da5b960bfeb1d23fde0 NOW.\n",
      "WARNING:pystan:81 of 2000 iterations ended with a divergence (4.05 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:Chain 3: E-BFMI = 0.149\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n"
     ]
    }
   ],
   "source": [
    "import pystan\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# model specification - this can be read directly off a file by PyStan (see docs)\n",
    "model_code = \"\"\"\n",
    "data {\n",
    "  int<lower=0> J;\n",
    "  real y[J];\n",
    "  real<lower=0> sigma[J];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real mu;\n",
    "  real theta[J];\n",
    "  real<lower=0> tau;\n",
    "}\n",
    "\n",
    "model {\n",
    "  mu ~ normal(0, 5);\n",
    "  tau ~ cauchy(0, 5);\n",
    "  theta ~ normal(mu, tau);\n",
    "  y ~ normal(theta, sigma);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# data to suppply to the sampler\n",
    "model_data = dict(\n",
    "    J=8,\n",
    "    y=[28,  8, -3,  7, -1,  1, 18, 12], \n",
    "    sigma=[15, 10, 16, 11,  9, 11, 10, 18],\n",
    ")\n",
    "\n",
    "# compile model\n",
    "sm = pystan.StanModel(model_code=model_code)\n",
    "\n",
    "# Train the model and generate samples\n",
    "fit = sm.sampling(\n",
    "    data=model_data,\n",
    "    iter=1000,\n",
    "    chains=4,\n",
    "    warmup=500,\n",
    "    thin=1,\n",
    "    control=dict(adapt_delta=0.8),\n",
    "    seed=101\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb66825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>se_mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>n_eff</th>\n",
       "      <th>Rhat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu</th>\n",
       "      <td>8.011503</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>4.451974</td>\n",
       "      <td>-0.648883</td>\n",
       "      <td>5.039417</td>\n",
       "      <td>7.950923</td>\n",
       "      <td>11.005299</td>\n",
       "      <td>16.774943</td>\n",
       "      <td>202.140699</td>\n",
       "      <td>1.022306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[1]</th>\n",
       "      <td>9.667488</td>\n",
       "      <td>0.326659</td>\n",
       "      <td>6.253741</td>\n",
       "      <td>-0.861679</td>\n",
       "      <td>5.649364</td>\n",
       "      <td>9.105090</td>\n",
       "      <td>12.805034</td>\n",
       "      <td>24.874256</td>\n",
       "      <td>366.513844</td>\n",
       "      <td>1.006775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[2]</th>\n",
       "      <td>8.094177</td>\n",
       "      <td>0.271956</td>\n",
       "      <td>5.399693</td>\n",
       "      <td>-2.054649</td>\n",
       "      <td>4.485995</td>\n",
       "      <td>7.963993</td>\n",
       "      <td>11.433316</td>\n",
       "      <td>19.309956</td>\n",
       "      <td>394.220558</td>\n",
       "      <td>1.005319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[3]</th>\n",
       "      <td>7.342270</td>\n",
       "      <td>0.325473</td>\n",
       "      <td>6.118242</td>\n",
       "      <td>-5.701914</td>\n",
       "      <td>3.626793</td>\n",
       "      <td>7.478395</td>\n",
       "      <td>11.485344</td>\n",
       "      <td>18.995560</td>\n",
       "      <td>353.364791</td>\n",
       "      <td>1.011964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[4]</th>\n",
       "      <td>7.892158</td>\n",
       "      <td>0.268866</td>\n",
       "      <td>5.726444</td>\n",
       "      <td>-3.530565</td>\n",
       "      <td>4.389887</td>\n",
       "      <td>7.750657</td>\n",
       "      <td>11.341168</td>\n",
       "      <td>19.213511</td>\n",
       "      <td>453.627152</td>\n",
       "      <td>1.013458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[5]</th>\n",
       "      <td>6.337915</td>\n",
       "      <td>0.360894</td>\n",
       "      <td>5.507145</td>\n",
       "      <td>-5.235362</td>\n",
       "      <td>2.987551</td>\n",
       "      <td>6.629630</td>\n",
       "      <td>10.101849</td>\n",
       "      <td>16.266123</td>\n",
       "      <td>232.859036</td>\n",
       "      <td>1.019186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[6]</th>\n",
       "      <td>7.091334</td>\n",
       "      <td>0.336453</td>\n",
       "      <td>5.573763</td>\n",
       "      <td>-5.034182</td>\n",
       "      <td>3.692490</td>\n",
       "      <td>7.150886</td>\n",
       "      <td>10.881964</td>\n",
       "      <td>17.533419</td>\n",
       "      <td>274.441023</td>\n",
       "      <td>1.014592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[7]</th>\n",
       "      <td>9.420656</td>\n",
       "      <td>0.304090</td>\n",
       "      <td>5.554803</td>\n",
       "      <td>-0.630173</td>\n",
       "      <td>5.730028</td>\n",
       "      <td>9.078182</td>\n",
       "      <td>12.626336</td>\n",
       "      <td>21.262046</td>\n",
       "      <td>333.683169</td>\n",
       "      <td>1.008134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theta[8]</th>\n",
       "      <td>8.500329</td>\n",
       "      <td>0.306907</td>\n",
       "      <td>6.482098</td>\n",
       "      <td>-4.160689</td>\n",
       "      <td>4.533094</td>\n",
       "      <td>8.334522</td>\n",
       "      <td>12.109275</td>\n",
       "      <td>22.559455</td>\n",
       "      <td>446.086254</td>\n",
       "      <td>1.007343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau</th>\n",
       "      <td>4.097441</td>\n",
       "      <td>0.331508</td>\n",
       "      <td>3.283590</td>\n",
       "      <td>0.519690</td>\n",
       "      <td>1.705793</td>\n",
       "      <td>3.222751</td>\n",
       "      <td>5.540268</td>\n",
       "      <td>12.619995</td>\n",
       "      <td>98.109513</td>\n",
       "      <td>1.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-14.864661</td>\n",
       "      <td>0.872109</td>\n",
       "      <td>6.407753</td>\n",
       "      <td>-26.396528</td>\n",
       "      <td>-19.341930</td>\n",
       "      <td>-15.212958</td>\n",
       "      <td>-10.399321</td>\n",
       "      <td>-1.781528</td>\n",
       "      <td>53.984654</td>\n",
       "      <td>1.074173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean   se_mean        sd       2.5%        25%        50%  \\\n",
       "mu         8.011503  0.313131  4.451974  -0.648883   5.039417   7.950923   \n",
       "theta[1]   9.667488  0.326659  6.253741  -0.861679   5.649364   9.105090   \n",
       "theta[2]   8.094177  0.271956  5.399693  -2.054649   4.485995   7.963993   \n",
       "theta[3]   7.342270  0.325473  6.118242  -5.701914   3.626793   7.478395   \n",
       "theta[4]   7.892158  0.268866  5.726444  -3.530565   4.389887   7.750657   \n",
       "theta[5]   6.337915  0.360894  5.507145  -5.235362   2.987551   6.629630   \n",
       "theta[6]   7.091334  0.336453  5.573763  -5.034182   3.692490   7.150886   \n",
       "theta[7]   9.420656  0.304090  5.554803  -0.630173   5.730028   9.078182   \n",
       "theta[8]   8.500329  0.306907  6.482098  -4.160689   4.533094   8.334522   \n",
       "tau        4.097441  0.331508  3.283590   0.519690   1.705793   3.222751   \n",
       "lp__     -14.864661  0.872109  6.407753 -26.396528 -19.341930 -15.212958   \n",
       "\n",
       "                75%      97.5%       n_eff      Rhat  \n",
       "mu        11.005299  16.774943  202.140699  1.022306  \n",
       "theta[1]  12.805034  24.874256  366.513844  1.006775  \n",
       "theta[2]  11.433316  19.309956  394.220558  1.005319  \n",
       "theta[3]  11.485344  18.995560  353.364791  1.011964  \n",
       "theta[4]  11.341168  19.213511  453.627152  1.013458  \n",
       "theta[5]  10.101849  16.266123  232.859036  1.019186  \n",
       "theta[6]  10.881964  17.533419  274.441023  1.014592  \n",
       "theta[7]  12.626336  21.262046  333.683169  1.008134  \n",
       "theta[8]  12.109275  22.559455  446.086254  1.007343  \n",
       "tau        5.540268  12.619995   98.109513  1.031000  \n",
       "lp__     -10.399321  -1.781528   53.984654  1.074173  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read into Pandas\n",
    "summary_dict = fit.summary()\n",
    "\n",
    "res_df = pd.DataFrame(\n",
    "    summary_dict['summary'], \n",
    "    columns=summary_dict['summary_colnames'], \n",
    "    index=summary_dict['summary_rownames']\n",
    ")\n",
    "\n",
    "# print some quantiles from our posterior \n",
    "res_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef031aa1",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B.\n",
    "Rubin. *Bayesian Data Analysis*. Chapman and Hall/CRC, New York, NY, 3\n",
    "edition, 2013.\n",
    "\n",
    "[2] Rubin, Donald B. “Estimation in Parallel Randomized Experiments.” Journal of Educational Statistics, vol. 6, no. 4, [Sage Publications, Inc., American Educational Research Association, American Statistical Association], 1981, pp. 377–401, https://doi.org/10.2307/1164617."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
